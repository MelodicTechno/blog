(window.webpackJsonp=window.webpackJsonp||[]).push([[39],{449:function(e,a,t){"use strict";t.r(a);var r=t(2),s=Object(r.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("blockquote",[a("p",[e._v("论文：Going deeper into action recognition: A survey")])]),e._v(" "),a("h3",{attrs:{id:"方法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#方法"}},[e._v("#")]),e._v(" 方法")]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"描述动作的方法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#描述动作的方法"}},[e._v("#")]),e._v(" 描述动作的方法")]),e._v(" "),a("h4",{attrs:{id:"local-representation-based-approaches"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#local-representation-based-approaches"}},[e._v("#")]),e._v(" Local representation based approaches")]),e._v(" "),a("ol",[a("li",[e._v("Interest point detection")]),e._v(" "),a("li",[e._v("Local descriptors\n"),a("ol",[a("li",[e._v("Edge and motion descriptors")]),e._v(" "),a("li",[e._v("Pixel pattern descriptors")]),e._v(" "),a("li",[e._v("From cuboids to trajectories")])])]),e._v(" "),a("li",[e._v("Aggregation\n上述内容较基础且年代久远，暂不详细整理。")])]),e._v(" "),a("h3",{attrs:{id:"用于动作检测的深度学习网络架构"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#用于动作检测的深度学习网络架构"}},[e._v("#")]),e._v(" 用于动作检测的深度学习网络架构")]),e._v(" "),a("hr"),e._v(" "),a("h4",{attrs:{id:"四种架构"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#四种架构"}},[e._v("#")]),e._v(" 四种架构")]),e._v(" "),a("ol",[a("li",[e._v("Spatiotemporal networks (时空网络)")]),e._v(" "),a("li",[e._v("Multiple stream networks (多流网络)")]),e._v(" "),a("li",[e._v("Deep generative networks (深度生成网络)")]),e._v(" "),a("li",[e._v("Temporal coherency networks (时间相干网络)\n以上中文翻译来自谷歌翻译，我认为中文名并不重要，不予深究。分类则应该是论文作者个人的观点。")])]),e._v(" "),a("h4",{attrs:{id:"spatiotemporal-networks"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#spatiotemporal-networks"}},[e._v("#")]),e._v(" Spatiotemporal networks")]),e._v(" "),a("h5",{attrs:{id:"零碎的记录"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#零碎的记录"}},[e._v("#")]),e._v(" 零碎的记录：")]),e._v(" "),a("ol",[a("li",[e._v("pooling和weight-sharing用于减少网络搜索的空间；")]),e._v(" "),a("li",[e._v("三维卷积在卷积的基础上增加了时序信息，使用三维的卷积核。三维卷积神经网络输入的视频的帧数是预先确定的；")]),e._v(" "),a("li",[e._v("在将时序信息输入(fusion)卷积网络的方法中，最大池化表现很好(吴恩达)；")]),e._v(" "),a("li",[e._v("slow fusion可以增强神经网络对时序的认知；在slow fusion中，相同的几个层接收几个连续的视频片段，输出再输入到全连接层，由此得以描述视频；")]),e._v(" "),a("li",[e._v("其他的fusion方法：early fusion: 逐帧特征加入到最后一层；Karpathy提出的方法：使用两个网络，能够增加精确度，同时减少需要学习的参数，因为每支网络能接受较小的输入，"),a("img",{attrs:{src:"/images/fffaa76abe5a2c3fcb339f265331df85_8_Figure_11_-1749198146.png",alt:"示例"}}),e._v("在这个示例中，fovea stream能注意到视频中央的区域，利用了摄像机的偏差，即兴趣点大多出现在视频中央；")]),e._v(" "),a("li",[e._v("Tran等人的工作: 只使用$3\\times3$ 的卷积核效果更好；")]),e._v(" "),a("li",[e._v("增加输入的时间的长度，同时结合使用具有不同对时间的意识的网络，能够提高神经网络的表现；")]),e._v(" "),a("li",[e._v("结合使用2D和1D的卷积核能减少3D卷积核对参数数量的需求；")]),e._v(" "),a("li",[e._v("Baccouche与Donahue等人：一系列卷积神经网络+LSTM，充分利用了时间信息；为了检测动作，Baccouche等人建议将三维卷积网络提取的特征输入到LSTM中；")]),e._v(" "),a("li",[e._v("Donahue等人：Long-term Recurrent Convolutional Network (LRCN)"),a("img",{attrs:{src:"images/fffaa76abe5a2c3fcb339f265331df85_9_Figure_14_-781209526.png",alt:"lrcn"}})])]),e._v(" "),a("hr"),e._v(" "),a("h4",{attrs:{id:"multiple-stream-networks"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#multiple-stream-networks"}},[e._v("#")]),e._v(" Multiple stream networks")]),e._v(" "),a("h5",{attrs:{id:"_1-simonyan与zisserman的双流网络"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-simonyan与zisserman的双流网络"}},[e._v("#")]),e._v(" 1. Simonyan与Zisserman的双流网络")]),e._v(" "),a("p",[e._v("结构如下：\n![[two_stream_network.png]] 这是两个并行的网络。")]),e._v(" "),a("ul",[a("li",[e._v("使用预训练的模型")]),e._v(" "),a("li",[e._v("输入时堆叠时序信息")]),e._v(" "),a("li",[e._v("有多个classification layer，每个在不同的训练集上训练，这是一种多任务学习\n双流网络使用softmax将两个流连接起来，在中间层融合可以表现得更好，同时减少需要学习的参数；在卷积层后融合可以减少对两个流的全连接层的需求；这个网络还可以进一步拓展：使用Fisher Vector，增加第三条支流来增加音频信号。双流网络中，播放的帧是唯一与动作相关的输入，这使双流网络无法捕获持续时间长的微小动作，将网络与手动提示结合起来可以改善这个问题。")])]),e._v(" "),a("hr"),e._v(" "),a("h4",{attrs:{id:"deep-generative-models"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#deep-generative-models"}},[e._v("#")]),e._v(" Deep generative models")]),e._v(" "),a("p",[e._v("几种模型如下：")]),e._v(" "),a("ol",[a("li",[e._v("Dynencoder")]),e._v(" "),a("li",[e._v("LSTM autoencoder model")]),e._v(" "),a("li",[e._v("Adversarial models")]),e._v(" "),a("li",[e._v("Temporal coherency networks")])]),e._v(" "),a("h5",{attrs:{id:"dynencoder"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dynencoder"}},[e._v("#")]),e._v(" Dynencoder")]),e._v(" "),a("p",[e._v("最基础的版本包含三层，第一层将输入$x_t$映射到隐藏$h_t$，第二层是预测层，基于当前的$h_t$预测$\\tilde{h}"),a("em",[e._v("{t+1}$，第三层使用预测的$\\tilde{h}")]),e._v("{t+1}$生成预测的帧$\\tilde{x}_{t+1}$。在合成动态纹理方面效果不错，可以理解成一种再现视频信息的简洁方法。")]),e._v(" "),a("h5",{attrs:{id:"lstm-dyencoder"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lstm-dyencoder"}},[e._v("#")]),e._v(" LSTM Dyencoder")]),e._v(" "),a("p",[e._v("构造如下：![[lstmaotoencoder.png]]")]),e._v(" "),a("h5",{attrs:{id:"adversarial-models"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#adversarial-models"}},[e._v("#")]),e._v(" Adversarial models")]),e._v(" "),a("p",[e._v("对抗网络")]),e._v(" "),a("h5",{attrs:{id:"temporal-coherency-networks"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#temporal-coherency-networks"}},[e._v("#")]),e._v(" Temporal coherency networks")]),e._v(" "),a("p",[e._v("一种弱监督学习的方法，用元组训练，判断动作是否连续。以Siamese Network为例：\n![[siames.png]]注意对时间上的连续性不一定意味着可靠性，比如插播广告时也是连续的，但显然广告与正片没有相关性。")]),e._v(" "),a("p",[e._v("Wang等人的工作：将动作划分为两个阶段来识别，将动作划分为前提（precondiction）和效果（effect），使用Siamese Network，构造如下：![[twophasecoherency.png]]\nRank pooling可以用来捕捉动作序列中的时序变化。")]),e._v(" "),a("hr"),e._v(" "),a("p",[e._v("本篇综述剩下的内容是对与不同网络表现的数值分析，上图：\n![[performance.png]]\n![[performance2.png]]")])])}),[],!1,null,null,null);a.default=s.exports}}]);